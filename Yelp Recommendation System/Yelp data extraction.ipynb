{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for extraction of data using YELP API and web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, time, json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_html(url):\n",
    "    \"\"\"\n",
    "    Return the raw HTML at the specified URL.\n",
    "\n",
    "    Args:\n",
    "        url (string): \n",
    "\n",
    "    Returns:\n",
    "        status_code (integer):\n",
    "        raw_html (string): the raw HTML content of the response, properly encoded according to the HTTP headers.\n",
    "    \"\"\"\n",
    "    article = requests.get(url)\n",
    "    art_content = article.text\n",
    "    \n",
    "    return article.status_code, art_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_api_key(filepath=\"api_key.txt\"):\n",
    "    \"\"\"\n",
    "    Read the Yelp API Key from file.\n",
    "    \n",
    "    Args:\n",
    "        filepath (string): File containing API Key\n",
    "    Returns:\n",
    "        api_key (string): The API Key\n",
    "    \"\"\"\n",
    "    \n",
    "    # Feel free to modify this function if you are storing the API Key differently\n",
    "    return Path(filepath).read_text().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yelp_search(api_key, query):\n",
    "    \"\"\"\n",
    "    Make an authenticated request to the Yelp API.\n",
    "\n",
    "    Args:\n",
    "        query (string): Search term\n",
    "\n",
    "    Returns:\n",
    "        total (integer): total number of businesses on Yelp corresponding to the query\n",
    "        businesses (list): list of dicts representing each business\n",
    "    \"\"\"\n",
    "    host = 'https://api.yelp.com'\n",
    "    path = '/v3/businesses/search'\n",
    "\n",
    "    url_params = {'location': query.replace(' ', '+')}\n",
    "\n",
    "    url = '{0}{1}'.format(host, quote(path.encode('utf8')))\n",
    "\n",
    "    headers = {\n",
    "        'Authorization': 'Bearer %s' % api_key,\n",
    "    }\n",
    "\n",
    "    response = requests.request('GET', url, headers=headers, params=url_params)\n",
    "    #print(response.json())\n",
    "    total = response.json()[\"total\"]\n",
    "    businesses = response.json()[\"businesses\"]\n",
    "\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_restaurants(api_key, query):\n",
    "    \"\"\"\n",
    "    Retrieve ALL the restaurants on Yelp for a given query.\n",
    "\n",
    "    Args:\n",
    "        query (string): Search term\n",
    "\n",
    "    Returns:\n",
    "        results (list): list of dicts representing each business\n",
    "    \"\"\"\n",
    "    host = 'https://api.yelp.com'\n",
    "    path = '/v3/businesses/search'\n",
    "    \n",
    "    limit = 20\n",
    "    offset = 1\n",
    "    \n",
    "    url_params = {'location': query.replace(' ', '+'), 'limit': limit, 'offset': offset}\n",
    "    \n",
    "    url = '{0}{1}'.format(host, quote(path.encode('utf8')))\n",
    "\n",
    "    headers = {\n",
    "        'Authorization': 'Bearer %s' % api_key,\n",
    "    }\n",
    "\n",
    "    response = requests.request('GET', url, headers=headers, params=url_params)\n",
    "    \n",
    "    total = response.json()[\"total\"]\n",
    "    businesses = response.json()[\"businesses\"]\n",
    "    \n",
    "    length = len(businesses)\n",
    "    \n",
    "    while length < total-1:\n",
    "        time.sleep(.800)\n",
    "        offset+=limit\n",
    "        \n",
    "        url_params = {'location': query.replace(' ', '+'), 'offset': offset}\n",
    "        response = requests.request('GET', url, headers=headers, params=url_params)\n",
    "        \n",
    "        if \"businesses\" in response.json().keys():\n",
    "            businesses.extend(response.json()[\"businesses\"])\n",
    "        else:\n",
    "            print(len(businesses),total,query)\n",
    "            return None\n",
    "            break\n",
    "    \n",
    "        length = len(businesses)\n",
    "        \n",
    "    return businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_api_response(data):\n",
    "    \"\"\"\n",
    "    Parse Yelp API results to extract restaurant URLs.\n",
    "    \n",
    "    Args:\n",
    "        data (list): list of restaurant data.\n",
    "\n",
    "    Returns:\n",
    "        (list): list of URLs as strings from the input list.\n",
    "    \"\"\"\n",
    "    url_list = []\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        url_dict = {}\n",
    "        url_dict[data[i][\"name\"]] = data[i][\"url\"]\n",
    "        url_list.append(url_dict)\n",
    "    \n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_page(html):\n",
    "    \"\"\"\n",
    "    Parse the reviews on a single page of a restaurant.\n",
    "    \n",
    "    Args:\n",
    "        html (string): String of HTML corresponding to a Yelp restaurant\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[Dict], int]: a tuple of two elements\n",
    "            first element: list of dictionaries corresponding to the extracted review information\n",
    "            second element: number of pages total.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    data = json.loads(soup.find_all(\"script\", type =\"application/ld+json\")[-1].string)\n",
    "    review_list = []\n",
    "    \n",
    "    for i in range(len(data['review'])):\n",
    "        review_list.append(dict())\n",
    "        review_list[i]['author'] = data['review'][i]['author']\n",
    "        review_list[i]['rating'] = float(data['review'][i]['reviewRating']['ratingValue'])\n",
    "        review_list[i]['date'] = data['review'][i]['datePublished']\n",
    "        review_list[i]['description'] = data['review'][i]['description']\n",
    "        \n",
    "    review_count = float(data['aggregateRating']['reviewCount'])\n",
    "    num_pages = review_count//20\n",
    "    \n",
    "    if review_count%20 != 0:\n",
    "        num_pages += 1\n",
    "        \n",
    "    return review_list,num_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reviews(url):\n",
    "    \"\"\"\n",
    "    Retrieve ALL of the reviews for a single restaurant on Yelp.\n",
    "\n",
    "    Parameters:\n",
    "        url (string): Yelp URL corresponding to the restaurant of interest.\n",
    "\n",
    "    Returns:\n",
    "        reviews (list): list of dictionaries containing extracted review information\n",
    "    \"\"\"\n",
    "    limit = 20\n",
    "    \n",
    "    restaurant = requests.get(url)\n",
    "    review_list, num_pages = parse_page(restaurant.text)\n",
    "    \n",
    "    page_number = 1\n",
    "    \n",
    "    try:\n",
    "        while page_number < num_pages:\n",
    "            time.sleep(.800)\n",
    "            restaurant = requests.get(url,params = {\"start\":limit*page_number})\n",
    "            review_list.extend(parse_page(restaurant.text)[0])\n",
    "            page_number += 1\n",
    "\n",
    "        return review_list\n",
    "    \n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def city_neighbourhoods(city_url):\n",
    "    \"\"\"\n",
    "    Parse Yelp API results to extract neighbourhoods in a city.\n",
    "    \n",
    "    Args:\n",
    "        data (string): URL of the city.\n",
    "\n",
    "    Returns:\n",
    "        (list): list of neighbourhoods as strings.\n",
    "    \"\"\"\n",
    "    html = retrieve_html(city_url)[1]\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    try:\n",
    "        neigh_dict = soup.find_all(\"script\",type=\"application/json\")[-3].string\n",
    "        neigh_dict = json.loads(neigh_dict[4:-3])\n",
    "        neigh_list = []\n",
    "\n",
    "        for i in range(len(neigh_dict['links'])):\n",
    "            neigh_list.append(neigh_dict['links'][i]['text'])\n",
    "\n",
    "        return neigh_list\n",
    "    except:\n",
    "        print(city_url)\n",
    "        return city_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restaurant_data(restaurants,neighbourhood = None):\n",
    "    \"\"\"\n",
    "    Extract useful information from Yelp API results.\n",
    "    \n",
    "    Args:\n",
    "        restaurants (list): Results obtained from yelp api.\n",
    "\n",
    "    Returns:\n",
    "        (list): list of extracted restaurant information as a dictionary.\n",
    "    \"\"\"\n",
    "    businesses = []\n",
    "    \n",
    "    for i in range(len(restaurants)):\n",
    "        business = {}\n",
    "        business[\"name\"] = restaurants[i][\"name\"]\n",
    "        business[\"review_count\"] = restaurants[i][\"review_count\"]\n",
    "        business[\"categories\"] = []\n",
    "        for j in range(len(restaurants[i][\"categories\"])):\n",
    "            business[\"categories\"].append(restaurants[i][\"categories\"][j][\"title\"])\n",
    "        business[\"latitude\"] = restaurants[i][\"coordinates\"][\"latitude\"]\n",
    "        business[\"longitude\"] = restaurants[i][\"coordinates\"][\"longitude\"]\n",
    "        business[\"rating\"] = restaurants[i][\"rating\"]\n",
    "        if 'price' in restaurants[i].keys():\n",
    "            business[\"price\"] = restaurants[i][\"price\"]\n",
    "        else:\n",
    "            business[\"price\"] = '$'\n",
    "        if neighbourhood is not None:\n",
    "            business[\"neighbourhood\"] = neighbourhood\n",
    "        business[\"city\"] = restaurants[i][\"location\"][\"city\"]\n",
    "        business[\"address\"] = restaurants[i][\"location\"][\"display_address\"]\n",
    "        \n",
    "        businesses.append(business)\n",
    "        \n",
    "    return businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cities_url(url):\n",
    "    \"\"\"\n",
    "    Extract city urls.\n",
    "    \n",
    "    Args:\n",
    "        url (string): Yelp City url\n",
    "\n",
    "    Returns:\n",
    "        (dict): State and their corresponding cities and urls as a dictionary.\n",
    "    \"\"\"\n",
    "    html = retrieve_html(url)[1]\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "    state_data = soup.find_all(\"script\",type = \"application/json\")[-1].string\n",
    "    state_data = json.loads(state_data[4:-3])\n",
    "    state_dict = {}\n",
    "    \n",
    "    for i in range(len(state_data[\"stateLinkLists\"])):\n",
    "        state_dict[state_data[\"stateLinkLists\"][i][\"title\"]] = {}\n",
    "        for j in range(len(state_data[\"stateLinkLists\"][i][\"links\"])):\n",
    "            state_dict[state_data[\"stateLinkLists\"][i][\"title\"]][state_data[\"stateLinkLists\"][i][\"links\"][j][\"text\"]] = state_data[\"stateLinkLists\"][i][\"links\"][j][\"uri\"]\n",
    "    \n",
    "    return state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def city_dict_creation(state_dict):\n",
    "    cities = {}\n",
    "    cities_with_neighborhood = {}\n",
    "    \n",
    "    for state in (state_dict.keys()):\n",
    "        for city in state_dict[state]:\n",
    "            html = retrieve_html(\"https://www.yelp.com/\" + state_dict[state][city])[1]\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            tag_p_list = soup.find_all(\"p\")\n",
    "            city_data = []\n",
    "            for k in tag_p_list:\n",
    "                if k.has_attr(\"class\") and (k[\"class\"][0] == \"counter-widget_num\" or k[\"class\"][0] == \"counter-widget_text\") :\n",
    "                    k = BeautifulSoup(str(k))\n",
    "                    tag = k.p\n",
    "                    city_data.append(tag.string.replace(\",\",\"\"))\n",
    "                    \n",
    "            if len(city_data) == 8:\n",
    "                cities[city] = {}\n",
    "                for m in range(len(city_data)):\n",
    "                    if m%2 == 0:\n",
    "                        cities[city][city_data[m+1]] = int(city_data[m])\n",
    "                cities[city][\"url\"] = \"https://www.yelp.com/\" + state_dict[state][city]\n",
    "                        \n",
    "            elif len(city_data) == 10:\n",
    "                cities_with_neighborhood[city] = {}\n",
    "                for m in range(len(city_data)):\n",
    "                    if m%2 == 0:\n",
    "                        cities_with_neighborhood[city][city_data[m+1]] = int(city_data[m])\n",
    "                cities_with_neighborhood[city][\"url\"] = \"https://www.yelp.com/\" + state_dict[state][city]\n",
    "                        \n",
    "    return cities,cities_with_neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(f_name,entity):\n",
    "    with open(f_name,\"w\") as file:\n",
    "        json.dump(entity,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(f_name):\n",
    "    with open(f_name,\"r\") as file:\n",
    "        entity = json.load(file)\n",
    "        \n",
    "    return entity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
